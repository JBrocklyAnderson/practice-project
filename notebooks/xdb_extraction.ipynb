{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Information from Exploit DB\n",
    "Despite Exploit DB not having an official API, a community-built API named `pyxploitdb` will help search through the Exploit DB by CVE. Documentation for this API is [available on GitHub](https://github.com/nicolasmf/pyxploit-db/wiki/How-to-use#searchcve). The package can only be installed with `pip`, so a separate environment was created in order to avoid future package conflicts with packages managed by `conda`. This makes integrating an extraction module into the pipeline from within the `src` directory a challenge that this notebook aims to address, since a different kernel can be selected to run it in isolation.\n",
    "\n",
    "The note takes the CVEs from the SCADA-filtered MITRE results and extracts a couple key pieces of information from Exploit DB. After having extracted this information, it provides a way to merge these CVEs into the cleaned-up proof-of-concept data attained from Nomi Sec's GitHub repository. This creates a robust dataset of CVEs that have exploit codes available that can be used as the backbone of the project's analysis.\n",
    "\n",
    "The first thing to do is to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pyxploitdb as pyx\n",
    "from pprint import pprint as pp\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, it makes sense to define a couple functions that can keep track of the data they're finding in Exploit DB. `process_cve` takes a single CVE from the SCADA-filtered MITRE data and uses it to call Exploit DB's API, whereas `run_xdb_extraction` loops over each CVE to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cve(cve_id):\n",
    "    # Call the searchCVE function\n",
    "    result = pyx.searchCVE(cve_id)\n",
    "\n",
    "    # Default data\n",
    "    poc_count = 0\n",
    "    earliest_date = pd.NaT\n",
    "\n",
    "    if result:\n",
    "        print('Result found!')\n",
    "        poc_count = len(result)\n",
    "        earliest_date = min(\n",
    "            result, key=lambda exploit: datetime.strptime(\n",
    "                exploit.date_published, '%Y-%m-%d'\n",
    "            )\n",
    "        )\n",
    "        earliest_date = earliest_date.date_published\n",
    "\n",
    "    return poc_count, earliest_date\n",
    "\n",
    "def run_xdb_extraction(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Grab our CVE IDs\n",
    "    cves = df['cve_id']\n",
    "    # Initialize an empty dataframe to append our results\n",
    "    results = []\n",
    "    # Loop through the CVEs and append the captured data\n",
    "    for cve in cves:\n",
    "        print(f'Processing CVE: {cve} ...')\n",
    "        poc_count, earliest_date = process_cve(cve)\n",
    "        results.append({\n",
    "            'cve_id': cve,\n",
    "            'exploit_count': poc_count,\n",
    "            'earliest_date': earliest_date,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook cell actually loads the CVEs and runs the API client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path='../../data/processed/mitre/cve/cve_cleaned.parquet')\n",
    "\n",
    "# Extract proof-of-concept data\n",
    "df = run_xdb_extraction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Exploit DB data for ease-of-use\n",
    "df.to_parquet(path='../../data/intermediate/exploits/xdb/xdb_extracted.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With proof-of-concept exploit code information successfully extracted from the database, the next step is to merge the DataFrame created from Exploit DB into the one extracted from the PoC-in-GitHub data. This helps us not simply combine all of the rows and drop duplicates based on the `cve_id`, but add together the total `exploit_count` from each dataset or retain the `earliest_date` of the exploit code for a given CVE ID should they overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into Exploit DB data\n",
    "xdb = pd.read_parquet(\n",
    "    path='../../data/intermediate/exploits/xdb/xdb_extracted.parquet'\n",
    ")\n",
    "# Load PoC-in-GitHub data\n",
    "poc = pd.read_parquet(\n",
    "    path='../../data/processed/exploits/poc/poc_cleaned.parquet'\n",
    ")\n",
    "# Load in KEV data\n",
    "kev = pd.read_parquet(\n",
    "    path='../../data/processed/cisa/kev/kev_processed.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge PoC-in-GitHub with Exploit DB data\n",
    "df = pd.merge(poc, xdb, on='cve_id', how='outer', suffixes=('_poc', '_xdb'))\n",
    "# Find the sum total of exploit codes for a given CVE ID\n",
    "df['exploit_count'] = (\n",
    "    df['exploit_count_poc'].fillna(0) + df['exploit_count_xdb'].fillna(0)\n",
    ")\n",
    "# Find the earliest date of an exploit code from either dataset for a given CVE\n",
    "df['earliest_date'] = df[['earliest_date_poc', 'earliest_date_xdb']].min(axis=1)\n",
    "# Drop intermediate columns created during merge\n",
    "df = df[['cve_id', 'exploit_count', 'earliest_date']]\n",
    "\n",
    "# Group by CVE and aggregate the sum exploit count and min date for duplicates\n",
    "df = df.groupby('cve_id').agg({\n",
    "    'exploit_count': 'sum',\n",
    "    'earliest_date': 'min'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to add the KEV CVEs into the mix as well. This requires a different strategy since the datapoints used in the previous merge do not exist in the KEV. This merge is simpler to accomplish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_merge\n",
       "left_only     5028\n",
       "right_only     753\n",
       "both           478\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge new DataFrame with KEV data\n",
    "df = pd.merge(df, kev, on='cve_id', how='outer', indicator=True)\n",
    "df['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the indicator, we can see that 478 CVEs were shared in common. We'll update the `exploit_count` attribute to `1` for those CVEs that came only from the KEV since we know that they have at least one exploit code that successfully exploits the vulnerability, but we won't touch the `exploit_count` of those CVEs that existed in both datasets since the merge ensured they adopted the proper counts from the get-go. Lastly, we'll perform a simple datatype transformation before saving the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cve_id converted to string!\n",
      "cve_name converted to string!\n",
      "cve_short_desc converted to string!\n",
      "required_action converted to string!\n",
      "notes converted to string!\n",
      "cwe_id converted to string!\n",
      "vendor converted to category!\n",
      "product converted to category!\n"
     ]
    }
   ],
   "source": [
    "from utils import convert_cols\n",
    "\n",
    "# Allow imports from outside the notebook directory\n",
    "src_path = os.path.abspath(os.path.join('../..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Update KEV CVE's exploit counts\n",
    "df.loc[df['_merge'] == 'right_only', 'exploit_count'] = 1\n",
    "# Update column types\n",
    "COL_TYPES = {\n",
    "    'string': [\n",
    "        'cve_id',\n",
    "        'cve_name',\n",
    "        'cve_short_desc',\n",
    "        'required_action',\n",
    "        'notes',\n",
    "        'cwe_id'\n",
    "    ],\n",
    "    'category': ['vendor', 'product']\n",
    "}\n",
    "df = convert_cols(df, COL_TYPES)\n",
    "# Drop the merge indicator\n",
    "df.drop(columns='_merge', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the merged and aggregated DataFrame for use in the rest of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(path='../../data/processed/composite/exploits_cleaned.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
